{
 "cells": [
  {
   "cell_type": "raw",
   "id": "729cf968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDFkHpdBLxdu",
    "outputId": "3413f462-d866-46f3-b78e-afe02c908991",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install torch\n",
    "!pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.12.1+cu116.html\n",
    "\n",
    "!pip install torch-geometric\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118e4e94",
   "metadata": {
    "id": "118e4e94"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import math\n",
    "import csv\n",
    "from csv import DictWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score, roc_auc_score,\n",
    "                              precision_score, recall_score, f1_score)\n",
    "\n",
    "import torch\n",
    "from torch.nn import (BCEWithLogitsLoss)\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import node2vec2 as n2v\n",
    "#from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GAE\n",
    "from torch_geometric.utils import (negative_sampling)\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040b28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets():\n",
    "    path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "    \n",
    "    isExist = os.path.exists(path + \"/graph_info.csv\")\n",
    "    \n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "        if args.dataset == 'ogb':\n",
    "            dataset = PygLinkPropPredDataset(name=args.dataset_name)\n",
    "            data = dataset[0]\n",
    "            split_edge = dataset.get_edge_split()\n",
    "            pos_train_edge = split_edge['train']['edge']\n",
    "            split_edge['train']['edge_neg'] = negative_sampling(data.edge_index, num_nodes=data.num_nodes,\n",
    "                num_neg_samples=pos_train_edge.size(0)).T\n",
    "\n",
    "            edge_weight = torch.ones(data.edge_index.size(1), dtype=int)\n",
    "            A = ssp.csr_matrix((edge_weight, (data.edge_index[0], data.edge_index[1])), \n",
    "                               shape=(data.num_nodes, data.num_nodes))\n",
    "            n_v = data.num_nodes\n",
    "            n_e = data.num_edges\n",
    "            num_subgraph_nodes = math.ceil((2*n_e/n_v)*(1+((2*n_e)/((n_v)*(n_v-1)))))#PLACN\n",
    "          \n",
    "\n",
    "        elif args.dataset == 'other':\n",
    "            \n",
    "            A, A_train, A_test, data_x, n_v, n_e, train_message_edges, test_message_edges,\\\n",
    "            edge_pos_train, edge_neg_train, edge_pos_test, edge_neg_test = data_loader()\n",
    "\n",
    "\n",
    "            data_train = Data(x = data_x, edge_index = train_message_edges, num_nodes = n_v)\n",
    "            data_test = Data(x = data_x, edge_index = test_message_edges, num_nodes = n_v)\n",
    "            \n",
    "            num_subgraph_nodes = math.ceil((2*n_e/n_v)*(1+((2*n_e)/((n_v)*(n_v-1)))))#PLACN\n",
    "            num_subgraph_nodes_primitive = num_subgraph_nodes + math.ceil(0.5*num_subgraph_nodes) + 2\n",
    "            \n",
    "            arrayOfA = A.toarray()\n",
    "            listOfA = arrayOfA.tolist()\n",
    "            \n",
    "            arrayOfA_train = A_train.toarray()\n",
    "            listOfA_train = arrayOfA_train.tolist()\n",
    "            \n",
    "            arrayOfA_test = A_test.toarray()\n",
    "            listOfA_test = arrayOfA_test.tolist()\n",
    "            \n",
    "            field_names = ['adj_matrix', 'adj_matrix_train', 'adj_matrix_test', 'data_x', 'num_vertices', 'num_edges',\n",
    "                           'edge_index', 'train_message_edges', 'test_message_edges', 'edge_pos_train', 'edge_neg_train',\n",
    "                           'edge_pos_test', 'edge_neg_test','num_subgraph_nodes']\n",
    "            \n",
    "            dict = {'adj_matrix':listOfA, 'adj_matrix_train':listOfA_train, 'adj_matrix_test':listOfA_test,\n",
    "                    'data_x':data_x.tolist(), 'num_vertices':n_v,  'num_edges':n_e,\n",
    "                    'train_message_edges':train_message_edges.tolist(), 'test_message_edges':test_message_edges.tolist(),\n",
    "                    'edge_pos_train':edge_pos_train.tolist(),\n",
    "                    'edge_neg_train':edge_neg_train.tolist(), 'edge_pos_test':edge_pos_test.tolist(),\n",
    "                    'edge_neg_test':edge_neg_test.tolist(), 'num_subgraph_nodes':num_subgraph_nodes}\n",
    "            \n",
    "            with open(path + \"/graph_info.csv\", mode='w') as f_object:\n",
    "                dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwriter_object.writeheader()\n",
    "                dictwriter_object.writerow(dict)\n",
    "                f_object.close()\n",
    "              \n",
    "            \n",
    "            field_names = ['id1', 'id2']\n",
    "            \n",
    "            edges = (torch.tensor(train_message_edges.tolist())).T\n",
    "            with open (path+'/%s_train_edges.csv'%(args.dataset_name), mode='w') as f_object:\n",
    "                dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwruter_object.writeheader()\n",
    "            for i in range(len(edges[0])):\n",
    "                dict = {'id1':edges[0][i].item(), 'id2':edges[1][i].item()}\n",
    "                with open (path+'/%s_train_edges.csv'%(args.dataset_name), mode='a') as f_object:\n",
    "                    dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                    dictwruter_object.writerow(dict)\n",
    "                    f_object.close()\n",
    "            \n",
    "            edges = (torch.tensor(test_message_edges.tolist())).T\n",
    "            with open (path+'/%s_test_edges.csv'%(args.dataset_name), mode='w') as f_object:\n",
    "                dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwruter_object.writeheader()\n",
    "            for i in range(len(edges[0])):\n",
    "                dict = {'id1':edges[0][i].item(), 'id2':edges[1][i].item()}\n",
    "                with open (path+'/%s_test_edges.csv'%(args.dataset_name), mode='a') as f_object:\n",
    "                    dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                    dictwruter_object.writerow(dict)\n",
    "                    f_object.close()\n",
    "                \n",
    "                \n",
    "        train_edges_x = np.concatenate([edge_pos_train.T[0],edge_neg_train.T[0]])\n",
    "        train_edges_y = np.concatenate([edge_pos_train.T[1],edge_neg_train.T[1]])\n",
    "        subgraph_primitive(A_train,train_edges_x , train_edges_y, num_nodes=num_subgraph_nodes_primitive, type_data='train' ) \n",
    "        \n",
    "        test_edges_x = np.concatenate([edge_pos_test.T[0],edge_neg_test.T[0]])\n",
    "        test_edges_y = np.concatenate([edge_pos_test.T[1],edge_neg_test.T[1]])\n",
    "        subgraph_primitive(A_test, test_edges_x, test_edges_y, num_nodes=num_subgraph_nodes_primitive, type_data='test' ) \n",
    "\n",
    "    else:\n",
    "        \n",
    "        if args.dataset == 'ogb':\n",
    "            \n",
    "            df = pd.read_csv(path+\"/graph_info.csv\")\n",
    "            \n",
    "        elif args.dataset == 'other':\n",
    "            \n",
    "            df = pd.read_csv(path+\"/graph_info.csv\")\n",
    "            A = ssp.csr_matrix(ast.literal_eval(df['adj_matrix'].dropna().values[0]))\n",
    "            A_train = ssp.csr_matrix(ast.literal_eval(df['adj_matrix_train'].dropna().values[0]))\n",
    "            A_test = ssp.csr_matrix(ast.literal_eval(df['adj_matrix_test'].dropna().values[0]))\n",
    "            data_x = torch.tensor(ast.literal_eval(df['data_x'].dropna().values[0]))\n",
    "            train_message_edges = torch.tensor(ast.literal_eval(df['train_message_edges'].dropna().values[0]))\n",
    "            test_message_edges = torch.tensor(ast.literal_eval(df['test_message_edges'].dropna().values[0]))\n",
    "            edge_pos_train = torch.tensor(ast.literal_eval(df['edge_pos_train'].dropna().values[0]))\n",
    "            edge_neg_train = torch.tensor(ast.literal_eval(df['edge_neg_train'].dropna().values[0]))\n",
    "            edge_pos_test = torch.tensor(ast.literal_eval(df['edge_pos_test'].dropna().values[0]))\n",
    "            edge_neg_test = torch.tensor(ast.literal_eval(df['edge_neg_test'].dropna().values[0]))\n",
    "            n_v = df['num_vertices'].dropna().values[0]\n",
    "            n_e = df['num_edges'].dropna().values[0]\n",
    "            num_subgraph_nodes = df['num_subgraph_nodes'].dropna().values[0]\n",
    "            num_nodes_primitive = num_subgraph_nodes + math.ceil(0.5*num_subgraph_nodes) + 2\n",
    "            \n",
    "            data_train = Data(x = data_x, edge_index = train_message_edges, num_nodes = n_v)\n",
    "            data_test = Data(x = data_x, edge_index = test_message_edges, num_nodes = n_v)\n",
    "            \n",
    "    if (args.subgraph_type == 'hhop' or args.subgraph_type == 'hhop-DIS'):\n",
    "        isExist = os.path.exists(path + \"/subgraphs_train_features.csv\")  \n",
    "        if not isExist:\n",
    "            subgraph_features(A, num_subgraph_nodes)\n",
    "    \n",
    "    elif args.subgraph_type == 'DIS':\n",
    "        isExist = os.path.exists(path + \"/subgraphs_train_features_dist.csv\")  \n",
    "        if not isExist:\n",
    "            subgraph_features(A, num_subgraph_nodes)\n",
    "\n",
    "    sub_feature_train_matrix, sub_adj_train_matrix = subgraph_matrixs(data_train, A_train, num_subgraph_nodes+2, 'train')\n",
    "    sub_feature_test_matrix, sub_adj_test_matrix = subgraph_matrixs(data_test, A_test, num_subgraph_nodes+2, 'test') \n",
    "\n",
    "    print(\"number of nodes:\", n_v)  \n",
    "    print(\"number of edges:\", n_e) \n",
    "    print(\"number of message edges in train:\", len(train_message_edges))\n",
    "    print(\"number of message edges in test:\", len(test_message_edges))\n",
    "    print(\"number of supervision edges in train:\", len(edge_pos_train))\n",
    "    print(\"number of supervision edges in test:\", len(edge_pos_test))\n",
    "    \n",
    "    print(\"number of subgraph nodes :\", num_subgraph_nodes)\n",
    "    \n",
    "\n",
    "    return(data_train, data_test, A, A_train, A_test, edge_pos_train,\n",
    "           edge_neg_train, edge_pos_test, edge_neg_test, num_subgraph_nodes,\n",
    "           sub_feature_train_matrix, sub_adj_train_matrix, \n",
    "           sub_feature_test_matrix, sub_adj_test_matrix, num_subgraph_nodes)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4813168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    \n",
    "    network_type = args.network_type\n",
    "    feature_type = args.feature_type\n",
    "\n",
    "    \n",
    "    print(\"load data...\")\n",
    "    if args.dataset == 'other':\n",
    "        file_path = \"dataset/\" + args.dataset_name + \".txt\"\n",
    "\n",
    "        #sample positive\n",
    "        positive_all = np.loadtxt(file_path, dtype=int, usecols=(0, 1))\n",
    "        if np.min(positive_all) == 1:\n",
    "            positive_all -= 1\n",
    "        np.random.shuffle(positive_all)\n",
    "        n = int(len(positive_all)*0.54)\n",
    "        positive = np.asarray(positive_all[:n])\n",
    "        supervision_edges_pos = positive\n",
    "        message_edges = np.asarray(positive_all[n:])\n",
    "\n",
    "        G = nx.Graph() if args.network_type == 0 else nx.DiGraph()\n",
    "        G.add_edges_from(positive_all)\n",
    "        print(G)\n",
    "        nodes_size = len(G.nodes()) #nodes size in the network\n",
    "        edge_size = len(G.edges())\n",
    "\n",
    "        # sample negative\n",
    "        negative_all = list(nx.non_edges(G))\n",
    "        if np.min(negative_all) == 1:\n",
    "            negative_all -= 1\n",
    "        np.random.shuffle(negative_all)\n",
    "        negative = np.asarray(negative_all[:len(positive)])\n",
    "        supervision_edges_neg = negative\n",
    "\n",
    "        test_size = int(len(positive) * args.test_ratio)\n",
    "        train_pos, test_pos = supervision_edges_pos[:-test_size], supervision_edges_pos[-test_size:]\n",
    "        train_neg, test_neg = supervision_edges_neg[:-test_size], supervision_edges_neg[-test_size:]\n",
    "\n",
    "        train_message_edges = message_edges\n",
    "        test_message_edges = np.concatenate([message_edges, train_pos])\n",
    "\n",
    "        #adj matrix\n",
    "        A = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "        A[positive_all[:, 0], positive_all[:, 1]] = 1\n",
    "\n",
    "        if network_type == 0:\n",
    "            A[positive_all[:, 1], positive_all[:, 0]] = 1\n",
    "\n",
    "\n",
    "        A_train = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "        A_train[train_message_edges[:, 0], train_message_edges[:, 1]] = 1\n",
    "\n",
    "\n",
    "        if network_type == 0:\n",
    "            A_train[train_message_edges[:, 1], train_message_edges[:, 0]] = 1\n",
    "\n",
    "\n",
    "        A_test = np.zeros([nodes_size, nodes_size], dtype=np.uint8)\n",
    "        A_test[test_message_edges[:, 0], test_message_edges[:, 1]] = 1\n",
    "        if network_type == 0:\n",
    "            A_test[test_message_edges[:, 1], test_message_edges[:, 0]] = 1\n",
    "\n",
    "        for i in range(nodes_size):\n",
    "            A_test[i,i] = 1\n",
    "            A_train[i,i] = 1\n",
    "            A[i,i] = 1\n",
    "\n",
    "        A = ssp.csr_matrix(A)    \n",
    "        A_test = ssp.csr_matrix(A_test) \n",
    "        A_train = ssp.csr_matrix(A_train)\n",
    "\n",
    "        path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "        #nodes feature\n",
    "        if feature_type == \"node2vec\":\n",
    "            embed = node2vec(A)\n",
    "\n",
    "            data_x = torch.tensor(embed)\n",
    "\n",
    "        elif feature_type == \"onehot\":\n",
    "            data_x = torch.diag(torch.ones(nodes_size))#ONE_HOT\n",
    "\n",
    "            \n",
    "    return(A ,A_train, A_test, data_x, nodes_size, edge_size,\n",
    "               torch.from_numpy(train_message_edges), torch.from_numpy(test_message_edges), \n",
    "               torch.from_numpy(train_pos), torch.from_numpy(train_neg),\n",
    "              torch.from_numpy(test_pos), torch.from_numpy(test_neg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8880f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_features(A, num_subgraph_nodes):\n",
    "    path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "     \n",
    "    embed = []\n",
    "    \n",
    "    if args.feature_type == 'node2vec':\n",
    "        df = pd.read_csv(path+'/%s_node2vec.csv'%(args.dataset_name))\n",
    "        f = df['node2vec'].dropna().values\n",
    "        \n",
    "        for i in f:\n",
    "            embed.append(torch.tensor(ast.literal_eval(i)))\n",
    "            \n",
    "    elif args.feature_type == 'onehot':\n",
    "        embed = torch.diag(torch.ones(nodes_size))\n",
    "\n",
    "    G = nx.from_scipy_sparse_array(A)\n",
    "    \n",
    "    num_nodes = num_subgraph_nodes\n",
    "    \n",
    "\n",
    "    #****************************train*******************************\n",
    "    df = pd.read_csv(path +'/subgraphs_train_info.csv')\n",
    "    nodes1 = df['nodes1'].dropna().values\n",
    "    nodes2 = df['nodes2'].dropna().values\n",
    "    \n",
    "    feature_vectors = []\n",
    "    subgraphs_A = []\n",
    "    subgraphs_n2v = []\n",
    "    subgraphs_nodes = []\n",
    "\n",
    "            \n",
    "    if (args.subgraph_type == 'hhop'or args.subgraph_type == 'hhop-DIS'):\n",
    "        path2 = path+'/subgraphs_train_features.csv'\n",
    "        \n",
    "        f = df['subgraph_nodes'].dropna().values\n",
    "        for i in f:\n",
    "            subgraphs_nodes.append((ast.literal_eval(i)))\n",
    "        \n",
    "        for i in subgraphs_nodes:\n",
    "            a=[]\n",
    "            for j in i:\n",
    "                a.append(embed[j].tolist())\n",
    "            subgraphs_n2v.append(a)\n",
    "        \n",
    "    elif args.subgraph_type == 'DIS':\n",
    "        path2 = path+'/subgraphs_train_features_dist.csv'\n",
    "        edges = torch.cat([torch.tensor([nodes1]), torch.tensor([nodes2])], dim=0).T\n",
    "        subgraphs_nodes = []\n",
    "        for index in range(len(edges)):\n",
    "            dis = []\n",
    "            dataf = []\n",
    "\n",
    "            embed_node1 = embed[edges[index][0].item()]\n",
    "            embed_node2 = embed[edges[index][1].item()]\n",
    "            embed_node_index = ((embed_node1+embed_node2)/2)\n",
    "\n",
    "\n",
    "            d_list = []\n",
    "            dataf = []\n",
    "\n",
    "            if args.dist_type == 'cos':\n",
    "                cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "\n",
    "            for i in range (len(embed)):\n",
    "                d=0\n",
    "                if i != edges[index][0]  and i != edges[index][1]:\n",
    "                    if args.dist_type == 'norm':\n",
    "                        d = (torch.norm((embed[i])-(embed_node_index))).item()\n",
    "\n",
    "                    elif args.dist_type == 'cos':\n",
    "                        d = (1-(cos(embed[i], embed_node_index).item()))\n",
    "\n",
    "                else:\n",
    "                    d = 0\n",
    "\n",
    "                d_list.append(d)\n",
    "                dataf.append([i,embed[i], d])\n",
    "\n",
    "            df = pd.DataFrame(dataf, columns=['node', 'embed', 'distance'])\n",
    "            df = df.sort_values('distance', ascending=True)\n",
    "\n",
    "            sub_nodes = []\n",
    "            nodes = ((df['node'].dropna().values)[:(num_subgraph_nodes+2)])\n",
    "            \n",
    "            for n in nodes:\n",
    "                sub_nodes.append(n)\n",
    "            sub_embed = torch.tensor(np.array([t.numpy() for t in (df['embed'].dropna().values)[:(num_subgraph_nodes+2)]]))\n",
    "            subgraphs_n2v.append(sub_embed)\n",
    "            subgraphs_nodes.append(sub_nodes)\n",
    "            \n",
    "        \n",
    "\n",
    "    #for subgraph\n",
    "    for subgraph_nodes in subgraphs_nodes:\n",
    "        \n",
    "        SA = (A[subgraph_nodes, :][:, subgraph_nodes]).toarray()\n",
    "        SA[0][1]=0.0\n",
    "        SA[1][0]=0.0\n",
    "\n",
    "        listOfsubgraphs_A = SA.tolist()\n",
    "        subgraphs_A.append(listOfsubgraphs_A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #save to csv    \n",
    "\n",
    "    field_names = ['node1', 'node2', 'sub_nodes' , 'sub_A', 'sub_node2vec_matrix']\n",
    "    with open (path2, mode='w') as f_object:\n",
    "        dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "        dictwruter_object.writeheader()\n",
    "\n",
    "    for i in range(len(nodes1)):\n",
    "        if (args.subgraph_type == 'hhop'or args.subgraph_type == 'hhop-DIS'):\n",
    "            dict = {'node1':nodes1[i], 'node2':nodes2[i], 'sub_nodes':subgraphs_nodes[i],\n",
    "                    'sub_A':subgraphs_A[i], 'sub_node2vec_matrix':subgraphs_n2v[i]}\n",
    "        elif args.subgraph_type == 'DIS':\n",
    "            dict = {'node1':nodes1[i], 'node2':nodes2[i], 'sub_nodes':subgraphs_nodes[i],\n",
    "                    'sub_A':subgraphs_A[i], 'sub_node2vec_matrix':subgraphs_n2v[i].tolist()}\n",
    "\n",
    "        with open (path2, mode='a') as f_object:\n",
    "            dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "            dictwruter_object.writerow(dict)\n",
    "            f_object.close()\n",
    "\n",
    "    #*******************************test******************\n",
    "    df = pd.read_csv(path +'/subgraphs_test_info.csv')\n",
    "    nodes1 = df['nodes1'].dropna().values\n",
    "    nodes2 = df['nodes2'].dropna().values\n",
    "    \n",
    "    feature_vectors = []\n",
    "    subgraphs_A = []\n",
    "    subgraphs_n2v = []\n",
    "    subgraphs_motif = []\n",
    "    subgraphs_RD = []\n",
    "    motif_matrix=[]\n",
    "            \n",
    "    if (args.subgraph_type == 'hhop'or args.subgraph_type == 'hhop-DIS'):\n",
    "        path2 = path+'/subgraphs_test_features.csv'\n",
    "        f = df['subgraph_nodes'].dropna().values\n",
    "        for i in f:\n",
    "            subgraphs_nodes.append((ast.literal_eval(i)))\n",
    "        \n",
    "        for i in subgraphs_nodes:\n",
    "            a=[]\n",
    "            for j in i:\n",
    "                a.append(embed[j].tolist())\n",
    "            subgraphs_n2v.append(a)\n",
    "\n",
    "    elif args.subgraph_type == 'DIS':\n",
    "        path2 = path+'/subgraphs_test_features_dist.csv'\n",
    "        edges = torch.cat([torch.tensor([nodes1]), torch.tensor([nodes2])], dim=0).T\n",
    "        \n",
    "        subgraphs_nodes = []\n",
    "        for index in range(len(edges)):\n",
    "            dis = []\n",
    "            dataf = []\n",
    "            \n",
    "            embed_node1 = embed[edges[index][0].item()]\n",
    "            embed_node2 = embed[edges[index][1].item()]\n",
    "            embed_node_index = ((embed_node1+embed_node2)/2)\n",
    "\n",
    "\n",
    "            d_list = []\n",
    "            dataf = []\n",
    "            \n",
    "\n",
    "            if args.dist_type == 'cos':\n",
    "                cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "\n",
    "            for i in range (len(embed)):\n",
    "                \n",
    "                if (i != edges[index][0]  or i != edges[index][1]):\n",
    "                    \n",
    "                    if args.dist_type == 'norm':\n",
    "                        d = (torch.norm((embed[i])-(embed_node_index))).item()\n",
    "\n",
    "                    elif args.dist_type == 'cos':\n",
    "                        d = (1-(cos(embed[i], embed_node_index).item()))\n",
    "\n",
    "                else:\n",
    "                    d = 0\n",
    "\n",
    "                d_list.append(d)\n",
    "                dataf.append([i,embed[i], d])\n",
    "            df = pd.DataFrame(dataf, columns=['node', 'embed', 'distance'])\n",
    "            df = df.sort_values('distance', ascending=True)\n",
    "            \n",
    "            sub_nodes = []\n",
    "            nodes = ((df['node'].dropna().values)[:(num_subgraph_nodes+2)])\n",
    "            \n",
    "            for n in nodes:\n",
    "                sub_nodes.append(n) \n",
    "            \n",
    "            sub_embed = torch.tensor(np.array([t.numpy() for t in (df['embed'].dropna().values)[:(num_subgraph_nodes+2)]]))\n",
    "            subgraphs_n2v.append(sub_embed)\n",
    "            subgraphs_nodes.append(sub_nodes)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    #A for subgraph\n",
    "    for subgraph_nodes in subgraphs_nodes:\n",
    "        \n",
    "        SA = (A[subgraph_nodes, :][:, subgraph_nodes]).toarray()\n",
    "        SA[0][1]=0.0\n",
    "        SA[1][0]=0.0\n",
    "\n",
    "        listOfsubgraphs_A = SA.tolist()\n",
    "        subgraphs_A.append(listOfsubgraphs_A)\n",
    "\n",
    "\n",
    "\n",
    "    #save to csv    \n",
    "\n",
    "    field_names = ['node1', 'node2', 'sub_nodes' , 'sub_A', 'sub_node2vec_matrix']\n",
    "    with open (path2, mode='w') as f_object:\n",
    "        dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "        dictwruter_object.writeheader()\n",
    "\n",
    "    for i in range(len(nodes1)):\n",
    "        if (args.subgraph_type == 'hhop'or args.subgraph_type == 'hhop-DIS'):\n",
    "            dict = {'node1':nodes1[i], 'node2':nodes2[i], 'sub_nodes':subgraphs_nodes[i],\n",
    "                    'sub_A':subgraphs_A[i], 'sub_node2vec_matrix':subgraphs_n2v[i]}\n",
    "        elif args.subgraph_type == 'DIS':\n",
    "            dict = {'node1':nodes1[i], 'node2':nodes2[i], 'sub_nodes':subgraphs_nodes[i],\n",
    "                    'sub_A':subgraphs_A[i], 'sub_node2vec_matrix':subgraphs_n2v[i].tolist()}\n",
    "\n",
    "        with open (path2, mode='a') as f_object:\n",
    "            dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "            dictwruter_object.writerow(dict)\n",
    "            f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09ff430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subgraph_matrixs(data , A, num_nodes, type_sub):\n",
    "    \n",
    "    path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "    embed = []\n",
    "    sub_nodes = []\n",
    "    adj = []\n",
    "    embed_matrix = []\n",
    "    \n",
    "    if args.subgraph_type == 'DIS':\n",
    "        df2 = pd.read_csv(path+ \"/subgraphs_%s_features_dist.csv\" %(type_sub))\n",
    "        \n",
    "        if args.feature_type== 'node2vec':\n",
    "            \n",
    "            f = df2['sub_nodes'].dropna().values\n",
    "            for i in f:\n",
    "                sub_nodes.append(torch.tensor(ast.literal_eval(i)))\n",
    "            \n",
    "            embed_matrix = torch.zeros([len(sub_nodes),num_nodes,args.n2v_dim])  \n",
    "            f = df2['sub_node2vec_matrix'].dropna().values\n",
    "            for index in range(len(f)):\n",
    "                e = torch.tensor(ast.literal_eval(f[index]))\n",
    "                if len(torch.tensor(ast.literal_eval(f[index])))< num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(e)), args.n2v_dim])\n",
    "                    e = torch.cat([e,x])\n",
    "                embed_matrix[index] = (e)\n",
    "                \n",
    "            adj_matrix = torch.zeros([len(sub_nodes),num_nodes,num_nodes])    \n",
    "            f = df2['sub_A'].dropna().values\n",
    "            for index in range(len(f)):    \n",
    "                matrix = (torch.tensor(ast.literal_eval(f[index])))\n",
    "                \n",
    "                if len(matrix)<num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(matrix)), num_nodes])\n",
    "                    y = torch.zeros([len(matrix), (num_nodes-len(matrix))])\n",
    "                    matrix= torch.cat((matrix,y),dim=1)\n",
    "                    matrix = torch.cat((matrix,x),dim=0)\n",
    "                adj_matrix[index] = (matrix)\n",
    "\n",
    "    elif args.subgraph_type == 'hhop-DIS':\n",
    "        num_nodes = num_nodes-2\n",
    "        num_nodes = num_nodes + math.ceil(0.5*num_nodes) + 2\n",
    "        \n",
    "        df2 = pd.read_csv(path+ \"/subgraphs_%s_features.csv\" %(type_sub))\n",
    "         \n",
    "        if args.feature_type== 'node2vec':\n",
    "\n",
    "            f = df2['sub_nodes'].dropna().values\n",
    "            for i in f:\n",
    "                sub_nodes.append(torch.tensor(ast.literal_eval(i)))\n",
    "            \n",
    "            embed_matrix = torch.zeros([len(sub_nodes),num_nodes,args.n2v_dim]) \n",
    "            f = df2['sub_node2vec_matrix'].dropna().values\n",
    "            for index in range(len(f)):\n",
    "                e = torch.tensor(ast.literal_eval(f[index]))\n",
    "                if len(torch.tensor(ast.literal_eval(f[index])))< num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(e)), args.n2v_dim])\n",
    "                    e = torch.cat([e,x])\n",
    "                embed_matrix[index] = (e)\n",
    "            \n",
    "            adj_matrix = torch.zeros([len(sub_nodes),num_nodes,num_nodes])    \n",
    "            f = df2['sub_A'].dropna().values\n",
    "            for index in range(len(f)):    \n",
    "                \n",
    "                matrix = (torch.tensor(ast.literal_eval(f[index])))\n",
    "                \n",
    "                if len(matrix)<num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(matrix)), num_nodes])\n",
    "                    y = torch.zeros([len(matrix), (num_nodes-len(matrix))])\n",
    "                    matrix= torch.cat((matrix,y),dim=1)\n",
    "                    matrix = torch.cat((matrix,x),dim=0)\n",
    "                adj_matrix[index] = (matrix)\n",
    "            \n",
    "                \n",
    "    elif args.subgraph_type == 'hhop':\n",
    "        df2 = pd.read_csv(path+ \"/subgraphs_%s_features.csv\" %(type_sub))\n",
    "        \n",
    "        if args.feature_type== 'node2vec':\n",
    "\n",
    "            f = df2['sub_nodes'].dropna().values\n",
    "            for i in f:\n",
    "                sub_nodes.append(torch.tensor(ast.literal_eval(i))[:num_nodes])\n",
    "            \n",
    "            embed_matrix = torch.zeros([len(sub_nodes),num_nodes,args.n2v_dim]) \n",
    "            f = df2['sub_node2vec_matrix'].dropna().values\n",
    "            for index in range(len(f)):\n",
    "                e = torch.tensor(ast.literal_eval(f[index]))\n",
    "                if len(torch.tensor(ast.literal_eval(f[index])))< num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(e)), args.n2v_dim])\n",
    "                    e = torch.cat([e,x])\n",
    "                embed_matrix[index] = (e)[:num_nodes]\n",
    "            \n",
    "            adj_matrix = torch.zeros([len(sub_nodes),num_nodes,num_nodes]) \n",
    "            f = df2['sub_A'].dropna().values\n",
    "            for index in range(len(f)):\n",
    "                matrix = (torch.tensor(ast.literal_eval(f[index])))\n",
    "                \n",
    "                if len(matrix)<num_nodes:\n",
    "                    x = torch.zeros([(num_nodes-len(matrix)), num_nodes])\n",
    "                    y = torch.zeros([len(matrix), (num_nodes-len(matrix))])\n",
    "                    matrix= torch.cat((matrix,y),dim=1)\n",
    "                    matrix = torch.cat((matrix,x),dim=0)\n",
    "                        \n",
    "                a = []\n",
    "                for j in range(num_nodes):\n",
    "                    a.append((matrix[j][:num_nodes]).tolist())\n",
    "                \n",
    "                adj_matrix[index] = (torch.tensor(a))\n",
    "    \n",
    "    sub_nodes = np.array(sub_nodes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    return(embed_matrix, adj_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5287d188",
   "metadata": {
    "id": "5287d188"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc3c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_len(G,i,j):\n",
    "    try:\n",
    "        n=nx.shortest_path_length(G,i,j)\n",
    "    except nx.NetworkXNoPath:\n",
    "        n = len(G)+1\n",
    "\n",
    "    return(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce73a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a834d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "\n",
    "    accu = 0.0\n",
    "\n",
    "    pred_label = pred.ge(0.5).to(pred.device)\n",
    "    accu = torch.sum(pred_label == label) / label.shape[0]\n",
    "    accu = accu.item()\n",
    "\n",
    "    return round(accu,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a0544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5).to(pred.device)\n",
    "    recall = recall_score(label, pred_label)\n",
    "\n",
    "    return round(recall,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219e730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5).to(pred.device)\n",
    "    precision = precision_score(label, pred_label)\n",
    "\n",
    "    return round(precision,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a3265f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(pred, label):\n",
    "\n",
    "    pred_label = pred.ge(0.5).to(pred.device) \n",
    "    f1 = f1_score(label, pred_label)\n",
    "\n",
    "    return round(f1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2c2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_c(pred, label):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    pyplot.plot(recall, precision, marker='.', label='Logistic')\n",
    "\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "\n",
    "    pyplot.legend()\n",
    "\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be43ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(pred, label):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    \n",
    "    return (precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02bcf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_AUC(pred, label):\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(label, pred)\n",
    "    \n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    return round(pr_auc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0631eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(pred, label):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label, pred)\n",
    "    return(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ede9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(pred, label):\n",
    "    \n",
    "    auc = roc_auc_score(label, pred)\n",
    "\n",
    "    return round(auc,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8fe5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(pred, label):\n",
    "    \n",
    "    avg_precision = average_precision_score(label, pred)\n",
    "    \n",
    "    return round(avg_precision,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612d8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node2vec_embeddings(A, emd_size=128, negative_injection=False, train_neg=None):\n",
    "    if negative_injection:\n",
    "        row, col = train_neg\n",
    "        A = A.copy()\n",
    "        A[row, col] = 1  # inject negative train\n",
    "        A[col, row] = 1  # inject negative train\n",
    "    nx_G = nx.from_scipy_sparse_array(A)\n",
    "    G = n2v.Graph(nx_G, is_directed=False, p=1, q=1)\n",
    "    G.preprocess_transition_probs()\n",
    "    walks = G.simulate_walks(num_walks=10, walk_length=80)\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, vector_size=emd_size, window=10, min_count=0, sg=1, \n",
    "            workers=8, epochs=1)\n",
    "    wv = model.wv\n",
    "    embeddings = np.zeros([A.shape[0], emd_size], dtype='float32')\n",
    "    sum_embeddings = 0\n",
    "    empty_list = []\n",
    "    for i in range(A.shape[0]):\n",
    "        if str(i) in wv:\n",
    "            embeddings[i] = wv.word_vec(str(i))\n",
    "            sum_embeddings += embeddings[i]\n",
    "        else:\n",
    "            empty_list.append(i)\n",
    "    mean_embedding = sum_embeddings / (A.shape[0] - len(empty_list))\n",
    "    embeddings[empty_list] = mean_embedding\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e2a7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec(A):\n",
    "    embed = generate_node2vec_embeddings(A, args.n2v_dim)\n",
    "    data_x = torch.tensor(np.array(embed))\n",
    "    \n",
    "    path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "    \n",
    "    field_names = ['node2vec']\n",
    "    with open (path+'/%s_node2vec.csv'%(args.dataset_name), mode='w') as f_object:\n",
    "        dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "        dictwruter_object.writeheader()\n",
    "    \n",
    "    for i in range(len(data_x)):\n",
    "        dict = {'node2vec':(data_x[i]).tolist()}\n",
    "\n",
    "        with open (path+'/%s_node2vec.csv'%(args.dataset_name), mode='a') as f_object:\n",
    "            dictwruter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "            dictwruter_object.writerow(dict)\n",
    "            f_object.close()\n",
    "    return(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f80e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neighbors(fringe, A, outgoing=True):\n",
    "\n",
    "    if outgoing:\n",
    "        res = (set(A[list(fringe)].indices))\n",
    "    else:\n",
    "        res = set(A[:, list(fringe)].indices)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b941a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_hop_subgraph(num_hops, A, src, dst=None, num_nodes=0 ,sample_ratio=1.0, \n",
    "                   max_nodes_per_hop=None, node_features=None, \n",
    "                   y=1, directed=False, A_csc=None):\n",
    "    # Extract the k-hop enclosing subgraph around link (src, dst) from A. \n",
    "    if dst == None:\n",
    "        src = src.item()\n",
    "        nodes = [src]\n",
    "        visited = set([src])\n",
    "        fringe = set([src])\n",
    "        for dist in range(1, num_hops+1):\n",
    "            if not directed:\n",
    "                fringe = neighbors(fringe, A)\n",
    "            else:\n",
    "                out_neighbors = neighbors(fringe, A)\n",
    "                in_neighbors = neighbors(fringe, A_csc, False)\n",
    "                fringe = out_neighbors.union(in_neighbors)\n",
    "            fringe = fringe - visited\n",
    "            visited = visited.union(fringe)\n",
    "            if sample_ratio < 1.0:\n",
    "                fringe = random.sample(fringe, int(sample_ratio*len(fringe)))\n",
    "            if max_nodes_per_hop is not None:\n",
    "                if max_nodes_per_hop < len(fringe):\n",
    "                    fringe = random.sample(fringe, max_nodes_per_hop)\n",
    "            if len(fringe) == 0:\n",
    "                break\n",
    "            if num_nodes != 0:\n",
    "                if len(nodes + list(fringe)) > num_nodes:\n",
    "                    x = len(nodes + list(fringe)) - num_nodes\n",
    "                    nodes = nodes + list(fringe)[:-x]\n",
    "                    break\n",
    "                    \n",
    "                if len(nodes) == num_nodes:\n",
    "                    break\n",
    "                    \n",
    "            nodes = nodes + list(fringe)\n",
    "            \n",
    "            \n",
    "        subgraph = A[nodes, :][:, nodes]\n",
    "\n",
    "        # Remove target link between the subgraph.\n",
    "        subgraph[0, 1] = 0\n",
    "        subgraph[1, 0] = 0\n",
    "\n",
    "        if node_features is not None:\n",
    "            node_features = node_features[nodes]\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        src = src.item()\n",
    "        dst = dst.item()\n",
    "        nodes = [src, dst]\n",
    "        visited = set([src, dst])\n",
    "        fringe = set([src, dst])\n",
    "        for dist in range(1, num_hops+1):\n",
    "            if not directed:\n",
    "                fringe = neighbors(fringe, A)\n",
    "            else:\n",
    "                out_neighbors = neighbors(fringe, A)\n",
    "                in_neighbors = neighbors(fringe, A_csc, False)\n",
    "                fringe = out_neighbors.union(in_neighbors)\n",
    "            fringe = fringe - visited\n",
    "            visited = visited.union(fringe)\n",
    "            if sample_ratio < 1.0:\n",
    "                fringe = random.sample(fringe, int(sample_ratio*len(fringe)))\n",
    "            if max_nodes_per_hop is not None:\n",
    "                if max_nodes_per_hop < len(fringe):\n",
    "                    fringe = random.sample(fringe, max_nodes_per_hop)\n",
    "            if len(fringe) == 0:\n",
    "                break\n",
    "                \n",
    "            if num_nodes != 0:\n",
    "                if len(nodes + list(fringe)) > num_nodes:\n",
    "                    x = len(nodes + list(fringe)) - num_nodes\n",
    "                    nodes = nodes + list(fringe)[:-x]\n",
    "                    break\n",
    "                    \n",
    "                if len(nodes) == num_nodes:\n",
    "                    break\n",
    "                    \n",
    "            nodes = nodes + list(fringe)\n",
    "            \n",
    "        subgraph = A[nodes, :][:, nodes]\n",
    "\n",
    "        # Remove target link between the subgraph.\n",
    "        subgraph[0, 1] = 0\n",
    "        subgraph[1, 0] = 0\n",
    "\n",
    "        if node_features is not None:\n",
    "            node_features = node_features[nodes]\n",
    "\n",
    "    return nodes, subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0532afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_primitive(A, nodes1, nodes2, num_nodes, type_data ):\n",
    "    \n",
    "    max_hop= args.max_hop\n",
    "    \n",
    "    path = \"data/%s/%s\" %(args.dataset, args.dataset_name)\n",
    "\n",
    "    \n",
    "    field_names = ['nodes1', 'nodes2', 'subgraph_nodes', 'subgraph_A']\n",
    "    \n",
    "    dict={}\n",
    "    for index in range(len(nodes1)): \n",
    "        isExist = os.path.exists(path+'/subgraphs_%s_info.csv'%(type_data))\n",
    "        \n",
    "        subgraph_nodes, subgraph_A = k_hop_subgraph(max_hop, A, nodes1[index], nodes2[index], num_nodes=num_nodes)\n",
    "        \n",
    "        dict = {'nodes1':subgraph_nodes[0], 'nodes2':subgraph_nodes[1], 'subgraph_nodes':subgraph_nodes, 'subgraph_A':subgraph_A}\n",
    "        if not isExist:\n",
    "            with open(path+'/subgraphs_%s_info.csv'%(type_data), mode='w') as f_object:\n",
    "                dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwriter_object.writeheader()\n",
    "                dictwriter_object.writerow(dict)\n",
    "                f_object.close()\n",
    "        else:\n",
    "            with open(path+'/subgraphs_%s_info.csv'%(type_data), mode='a') as f_object:\n",
    "                dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "                dictwriter_object.writerow(dict)\n",
    "                f_object.close()\n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d32c2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph2vec(A, embed, num_nodes):\n",
    "    \n",
    "    z_embed = torch.tensor(()).to(embed.device)\n",
    "    \n",
    "    matrix_embed = torch.zeros([len(embed),num_nodes+2,len(embed[0][0])])\n",
    "\n",
    "    if args.dist_type == 'cos':\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    for index in range(len(embed)):\n",
    "        dis = []\n",
    "\n",
    "\n",
    "        embed_node1 = (embed[index][0])\n",
    "        embed_node2 = (embed[index][1])\n",
    "        embed_node_index = ((embed_node1+embed_node2)/2)\n",
    "\n",
    "\n",
    "\n",
    "        d_list = []\n",
    "        dataf = []\n",
    "        for i in range (len(embed[index])):\n",
    "\n",
    "            if i != 0 and i != 1:\n",
    "\n",
    "                if args.dist_type == 'norm':\n",
    "                    d_list.append((torch.norm((embed[index][i])-(embed_node_index))).item())\n",
    "\n",
    "                elif args.dist_type == 'cos':\n",
    "                    d_list.append(1-(cos(embed[index][i], embed_node_index).item()))\n",
    "            else:\n",
    "                d_list.append(0)\n",
    "\n",
    "        d_list = torch.tensor((normalize([d_list])[0]).tolist())\n",
    "\n",
    "\n",
    "        for i in range (len(embed[index])):\n",
    "            dataf.append([embed[index][i], d_list[i]])\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(dataf, columns=['embed', 'distance'])\n",
    "        df = df.sort_values('distance', ascending=True)\n",
    "\n",
    "\n",
    "        subgraph_embed = df['embed'].dropna().values[:(num_nodes+2)]\n",
    "        distances = df['distance'].dropna().values[:(num_nodes+2)]\n",
    "\n",
    "\n",
    "        if args.subgraph_feature_type == 'NDP':\n",
    "\n",
    "            w_list = torch.zeros(len(distances))\n",
    "\n",
    "            for i in range(len(distances)):\n",
    "                d = distances[i].item()\n",
    "                if d<=0:\n",
    "                    w_list[i] = math.log((1-0.0001)/0.0001)\n",
    "                elif(d>=1):\n",
    "                    w_list[i] = math.log((1-0.9999)/0.9999)\n",
    "                else:\n",
    "                    w_list[i] = math.log((1-d)/d) #adaboost\n",
    "            w_list = w_list / (w_list.sum())\n",
    "\n",
    "            for i in range(len(distances)):\n",
    "                subgraph_embed[i] = subgraph_embed[i]*w_list[i]\n",
    "\n",
    "            s = (subgraph_embed.sum())\n",
    "\n",
    "            s = s.reshape([1,len(s)])\n",
    "            z_embed = torch.cat((z_embed,s), 0)\n",
    "\n",
    "\n",
    "\n",
    "        elif args.subgraph_feature_type == 'CNN':\n",
    "            matrix = torch.tensor(np.array([t.detach().numpy() for t in subgraph_embed]))\n",
    "\n",
    "            if len(matrix)< num_nodes+2:\n",
    "                x = torch.zeros([(num_nodes+2-len(matrix)), len(matrix[0])])\n",
    "                matrix = torch.cat([matrix,x])\n",
    "\n",
    "\n",
    "            matrix_embed[index] = matrix\n",
    "            z_embed = matrix_embed\n",
    "                \n",
    "    \n",
    "    return(z_embed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7401e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_dense(subgraph_A ):\n",
    "    A_sub = torch.tensor(())\n",
    "    coo =subgraph_A.tocoo()\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    A_sub_coo = torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense().to(A_sub.device)\n",
    "    A_sub_coo= A_sub_coo.reshape([1,len(A_sub_coo),len(A_sub_coo[0])])\n",
    "    A_sub = torch.cat((A_sub, A_sub_coo),0)\n",
    "    return(A_sub.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b765b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_edge_index(adj):\n",
    "    edge_index = adj.nonzero().t().contiguous()\n",
    "    return(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "997b8a33",
   "metadata": {
    "id": "997b8a33"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        \n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "    def forward(self, embed, adj):\n",
    "        \n",
    "        output = torch.tensor([]).to(adj.device)\n",
    "        for i in range(adj.shape[0]):\n",
    "            edge_index = adj[i].nonzero().t().contiguous()\n",
    "            x = embed[i]\n",
    "            for conv in self.convs[:-1]:\n",
    "                x = conv(x, edge_index)\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.convs[-1](x, edge_index)\n",
    "            x = x.reshape([1,len(x),len(x[0])])\n",
    "            output = torch.cat([output,x],0)\n",
    "        #print(x)\n",
    "        #print('end encoder')\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43d7b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac64569",
   "metadata": {
    "id": "dac64569"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, GNN_in_channels, GNN_hidden_channels, GNN_out_channels,\n",
    "                 GNN_num_layers, dropout):\n",
    "\n",
    "        super(GAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoders = torch.nn.ModuleList()\n",
    "        self.encoders.append(GCNConv(GNN_in_channels, GNN_hidden_channels))\n",
    "        for _ in range(GNN_num_layers - 2):\n",
    "            self.encoders.append(GCNConv(GNN_hidden_channels, GNN_hidden_channels))\n",
    "        self.encoders.append(GCNConv(GNN_hidden_channels, GNN_out_channels))\n",
    "        \n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for encoder in self.encoders:\n",
    "            encoder.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for encoder in self.encoders[:-1]:\n",
    "            x = encoder(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.encoders[-1](x, adj) \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad58aea3",
   "metadata": {
    "id": "ad58aea3"
   },
   "outputs": [],
   "source": [
    "def predictor_simple(z, edge_index):\n",
    "    z1 = (z[edge_index[0].long()])\n",
    "    z2 = (z[edge_index[1].long()])\n",
    "    logits = (z1 * z2).sum(dim=-1)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db420367",
   "metadata": {
    "id": "db420367"
   },
   "outputs": [],
   "source": [
    "class predictor_model(torch.nn.Module):\n",
    "    def __init__(self, linear_in_channels, linear_hidden_channels, linear_num_layers, dropout):\n",
    "        \n",
    "        super(predictor_model, self).__init__()        \n",
    "        \n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(3*linear_in_channels, 128))\n",
    "        \n",
    "        self.lins.append(torch.nn.Linear(128, linear_hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(linear_hidden_channels, linear_hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(linear_hidden_channels, 16))\n",
    "        \n",
    "        self.lins.append(torch.nn.Linear(16, 1))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "            \n",
    "    def forward(self, z):\n",
    "        results = torch.zeros(len(z), dtype=torch.float).to(z.device)\n",
    "        for lin in self.lins[:-1]:\n",
    "            z = lin(z)\n",
    "            z = F.relu(z)\n",
    "            z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "        z = self.lins[-1](z)\n",
    "        result = z\n",
    "\n",
    "        index = 0\n",
    "        for r in result:\n",
    "            results[index] = r[0] \n",
    "            index += 1\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e96f4f35",
   "metadata": {
    "id": "e96f4f35"
   },
   "outputs": [],
   "source": [
    "class subg2vec_model(torch.nn.Module):\n",
    "    def __init__(self, input_num, feture_dim, hidden_channels, out_channels, dropout):\n",
    "        \n",
    "        super(subg2vec_model, self).__init__()        \n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, (3,feture_dim), padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, (3,2), padding=1)\n",
    "        \n",
    "        input_num = input_num+2\n",
    "        \n",
    "        x = math.ceil((math.ceil((input_num-3+2)/2)-3+2)/2)\n",
    "        y = math.ceil((math.ceil((feture_dim-2+2)/2)-2+2)/2)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(int(0.5*x*y), hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return (x.squeeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2324ec67",
   "metadata": {
    "id": "2324ec67"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train( model, model_predictor, data, A, data_split,  train_subgraph_adj, train_subgraph_features,\n",
    "          optimizer, optimizer_predictor, batch_size, num_subgraph_nodes, model_subgraph=None, optimizer_subgraph=None):\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    model_predictor.train()\n",
    "    \n",
    "    if args.subgraph_feature_type == 'CNN':\n",
    "        model_subgraph.train()\n",
    "    \n",
    "    pos_train_edge = data_split['train']['edge'].to(data.x.device)\n",
    "    neg_train_edge = data_split['train']['edge_neg'].to(data.x.device)\n",
    "    \n",
    "    train_edge = (torch.cat((pos_train_edge, neg_train_edge), dim=0))\n",
    "    link_labels = (get_link_labels(pos_train_edge.T, neg_train_edge.T))\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    optimizer_predictor.zero_grad()\n",
    "    if args.subgraph_feature_type == 'CNN':\n",
    "        optimizer_subgraph.zero_grad()\n",
    "\n",
    "    \n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    train_loader = DataLoader(range(len(train_edge)), batch_size, shuffle=True)\n",
    "    pbar = tqdm(train_loader, ncols=70)\n",
    "    for perm in pbar:\n",
    "        \n",
    "        i += 1\n",
    "        sub_adj = train_subgraph_adj[perm].to(data.x.device)\n",
    "        sub_x = train_subgraph_features[perm].to(data.x.device)\n",
    "        labels = link_labels[perm].to(data.x.device)\n",
    "        \n",
    "        \n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            z = model.encode(sub_x, sub_adj)\n",
    "        else:\n",
    "            z = model(sub_x, sub_adj)\n",
    "        \n",
    "        z1 = (z[: ,0])\n",
    "        z2 = (z[: ,1])\n",
    "        \n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "        \n",
    "        if args.subgraph_feature_type == 'CNN':\n",
    "            sub_embed = subgraph2vec(A, z, num_subgraph_nodes ).to(data.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "\n",
    "\n",
    "        elif args.subgraph_feature_type == 'NDP':\n",
    "            z_sub = subgraph2vec(A, z, num_subgraph_nodes ).to(data.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "        \n",
    "        link_logits = model_predictor(z_embed).to(data.x.device) # predict with NN\n",
    "        \n",
    "\n",
    "      \n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            loss_autoencoder = model.recon_loss(z, data.edge_index.T)\n",
    "            loss_predict = BCEWithLogitsLoss()(link_logits, link_labels)\n",
    "            loss = loss_autoencoder+loss_predict\n",
    "            loss.backward()\n",
    "\n",
    "        else:\n",
    "            loss = BCEWithLogitsLoss()(link_logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model_predictor.parameters(), 2.0)\n",
    "        if args.subgraph_feature_type == 'CNN':\n",
    "            torch.nn.utils.clip_grad_norm_(model_subgraph.parameters(), 2.0)\n",
    "\n",
    "        num_examples = (link_logits.size(0))\n",
    "        total_loss += loss.item() * num_examples\n",
    "        total_examples += num_examples\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer_predictor.step()\n",
    "        if args.subgraph_feature_type == 'CNN':\n",
    "            optimizer_subgraph.step()\n",
    "\n",
    "    return (total_loss/ total_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "135bfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, model_predictor,  data_train, A_train, data_test, A_test, split_edge, train_subgraph_adj, train_subgraph_features,\n",
    "         test_subgraph_adj, test_subgraph_features, batch_size, num_subgraph_nodes,\n",
    "         model_subgraph=None, optimizer_subgraph=None, evaluator=False):\n",
    "    \n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    model_predictor.eval()\n",
    "    if args.subgraph_feature_type == 'CNN':\n",
    "        model_subgraph.eval()\n",
    "\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].T.to(data_train.x.device)\n",
    "    neg_train_edge = split_edge['train']['edge_neg'].T.to(data_train.x.device)\n",
    "    pos_test_edge = split_edge['test']['edge'].T.to(data_train.x.device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].T.to(data_train.x.device)\n",
    "\n",
    "    train_preds = []\n",
    "    train_labels = (get_link_labels(pos_train_edge, neg_train_edge)).to(data_train.x.device)\n",
    "    train_edge = torch.cat((pos_train_edge, neg_train_edge), dim=1)\n",
    "    \n",
    "    \n",
    "    for perm in DataLoader(range(train_edge.size(1)), batch_size, shuffle=False):\n",
    "        \n",
    "        sub_adj = train_subgraph_adj[perm].to(data_train.x.device)\n",
    "        sub_x = train_subgraph_features[perm].to(data_train.x.device)\n",
    "        \n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            z = model.encode(sub_x, sub_adj)\n",
    "        else:\n",
    "            z = model(sub_x, sub_adj)\n",
    "            \n",
    "        z1 = (z[: ,0])\n",
    "        z2 = (z[: ,1])\n",
    "        \n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "       \n",
    "        if args.subgraph_feature_type == 'CNN':\n",
    "            sub_embed = subgraph2vec(A_train, z, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "            \n",
    "        \n",
    "        elif args.subgraph_feature_type == 'NDP':\n",
    "            z_sub = subgraph2vec(A_train, z, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "           \n",
    " \n",
    "\n",
    "        train_preds += [model_predictor(z_embed)]\n",
    " \n",
    "    train_pred = torch.cat(train_preds, dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "    test_preds = []\n",
    "    test_labels = (get_link_labels(pos_test_edge, neg_test_edge)).to(data_train.x.device)\n",
    "    test_edge = torch.cat((pos_test_edge, neg_test_edge), dim=1)\n",
    "    for perm in DataLoader(range(test_edge.size(1)), batch_size, shuffle=False):\n",
    "        sub_adj = test_subgraph_adj[perm].to(data_train.x.device)\n",
    "        sub_x = test_subgraph_features[perm].to(data_train.x.device)\n",
    "        \n",
    "        if args.GNN_type == 'autoencoder':\n",
    "            z = model.encode(sub_x, sub_adj)\n",
    "        else:\n",
    "            z = model(sub_x, sub_adj)\n",
    "        \n",
    "        z1 = (z[: ,0])\n",
    "        z2 = (z[: ,1])\n",
    "        \n",
    "        z_nodes = torch.cat([z1,z2], dim=-1)\n",
    "        \n",
    "        \n",
    "        if args.subgraph_feature_type == 'CNN':\n",
    "            sub_embed = subgraph2vec(A_test, z, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_sub = model_subgraph(sub_embed)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "            \n",
    "        elif args.subgraph_feature_type == 'NDP':\n",
    "            z_sub = subgraph2vec(A_test, z, num_subgraph_nodes ).to(data_train.x.device)\n",
    "            z_embed = torch.cat((z_nodes,z_sub),-1)\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "        test_preds += [model_predictor(z_embed)]\n",
    "    test_pred = torch.cat(test_preds, dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    train_logits = train_pred.to(data_train.x.device)\n",
    "    test_logits = test_pred.to(data_train.x.device)\n",
    "    \n",
    "\n",
    "    \n",
    "    accu_train = accuracy(train_logits, train_labels)\n",
    "    accu_test = accuracy(test_logits, test_labels)\n",
    "    \n",
    "\n",
    "    recall_train = recall(train_logits, train_labels)\n",
    "    recall_test = recall(test_logits, test_labels)\n",
    "    \n",
    "    precision_train = precision(train_logits, train_labels)\n",
    "    precision_test = precision(test_logits, test_labels)\n",
    "    \n",
    "    f1_train = f1(train_logits, train_labels)\n",
    "    f1_test = f1(test_logits, test_labels)\n",
    "    \n",
    "    avg_precision_train = average_precision(train_logits, train_labels)\n",
    "    avg_precision_test = average_precision(test_logits, test_labels)\n",
    "    \n",
    "    roc_auc_train = roc(train_logits, train_labels)\n",
    "    roc_auc_test = roc(test_logits, test_labels)\n",
    "    \n",
    "    pr_auc_train = precision_recall_AUC(train_logits, train_labels)\n",
    "    pr_auc_test = precision_recall_AUC(test_logits, test_labels)\n",
    "    \n",
    "    precision_list_train, recall_list_train = precision_recall(train_logits, train_labels)\n",
    "    precision_list_train = precision_list_train.tolist()\n",
    "    recall_list_train = recall_list_train.tolist()\n",
    "    precision_list_test, recall_list_test = precision_recall(test_logits, test_labels)\n",
    "    precision_list_test = precision_list_test.tolist()\n",
    "    recall_list_test = recall_list_test.tolist()\n",
    "    \n",
    "    results = {'train': {'accu_train': accu_train, 'recall_train':recall_train, 'precision_train':precision_train, \n",
    "                        'f1_train':f1_train, 'avg_precision_train':avg_precision_train, 'pr_auc_train':pr_auc_train, \n",
    "                        'precision_list_train':precision_list_train, 'recall_list_train':recall_list_train,\n",
    "                        'pred': train_logits, 'y':train_labels, 'roc_auc_train':roc_auc_train}, \n",
    "                'test': {'accu_test': accu_test, 'recall_test':recall_test, 'precision_test':precision_test, \n",
    "                        'f1_test':f1_test, 'avg_precision_test':avg_precision_test, 'pr_auc_test':pr_auc_test,\n",
    "                        'precision_list_test':precision_list_test, 'recall_list_test':recall_list_test, \n",
    "                        'pred': test_logits, 'y':test_labels, 'roc_auc_test':roc_auc_test}}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0f6ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(device=0, dataset='other', dataset_name='karate', subgraph_type='hhop', dist_type='norm', \n",
    "                    network_type=0, feature_type='onehot', subgraph_feature_type='CNN',\n",
    "                    negative_injection=True, log_steps=1, GNN_type='gcn', GNN_num_layers=3, GNN_hidden_channels=128,\n",
    "                    GNN_out_channels=128, linear_num_layers=5, linear_hidden_channels=32, n2v_dim=128,\n",
    "                    subg2vec_hidden_channels=128, subg2vec_out_channels=128, max_hop=10, dropout=0.0, no_start_run=1,\n",
    "                    batch_size=50, lr=0.0001 ,epochs=300, eval_steps=300, test_ratio=0.1,runs=10,\n",
    "                    coefficient=50, label='dist'):\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='GAE_subgraph')\n",
    "    parser.add_argument('--device', type=int, default=device)\n",
    "    parser.add_argument('--dataset', type=str, default=dataset) #pyg, ogb, networkx\n",
    "    parser.add_argument('--dataset_name', type=str, default=dataset_name) #'ogbl-collab', 'football', 'FB15k-237',\n",
    "    #'karate', 'USAir', 'PB'\n",
    "    parser.add_argument('--subgraph_type', type=str, default=subgraph_type)\n",
    "    parser.add_argument('--dist_type', type=str, default=dist_type)#'norm', 'cos'\n",
    "    parser.add_argument('--network_type', type=int, default=network_type)#if directed -> 0, if undirected -> 1\n",
    "    parser.add_argument('--feature_type', type=str, default=feature_type)#'node2vec', 'onehot'\n",
    "    parser.add_argument('--subgraph_feature_type', type=str, default=subgraph_feature_type)#'CNN', 'NDP'\n",
    "    parser.add_argument('--n2v_dim', type=int, default=n2v_dim)\n",
    "    parser.add_argument('--negative_injection', type=bool, default=negative_injection)\n",
    "    parser.add_argument('--log_steps', type=int, default=log_steps)\n",
    "    #parser.add_argument('--use_node_embedding', action='store_true')\n",
    "    parser.add_argument('--GNN_type', type=str, default=GNN_type)#'gcn', 'sage', 'autoencoder'\n",
    "    parser.add_argument('--GNN_num_layers', type=int, default=GNN_num_layers)\n",
    "    parser.add_argument('--GNN_hidden_channels', type=int, default=GNN_hidden_channels)\n",
    "    parser.add_argument('--GNN_out_channels', type=int, default=GNN_out_channels)\n",
    "    parser.add_argument('--linear_num_layers', type=int, default=linear_num_layers)\n",
    "    parser.add_argument('--linear_hidden_channels', type=int, default=linear_hidden_channels)\n",
    "    parser.add_argument('--subg2vec_hidden_channels', type=int, default=subg2vec_hidden_channels)\n",
    "    parser.add_argument('--subg2vec_out_channels', type=int, default=subg2vec_out_channels)\n",
    "    parser.add_argument('--max_hop', type=int, default=max_hop)\n",
    "    parser.add_argument('--dropout', type=float, default=dropout)\n",
    "    parser.add_argument('--batch_size', type=int, default=batch_size)\n",
    "    parser.add_argument('--lr', type=float, default=lr)\n",
    "    parser.add_argument('--epochs', type=int, default=epochs)\n",
    "    parser.add_argument('--eval_steps', type=int, default=eval_steps)\n",
    "    parser.add_argument('--test_ratio', type=float, default=test_ratio)\n",
    "    parser.add_argument('--runs', type=int, default=runs)\n",
    "    parser.add_argument('--coefficient', type=int, default=coefficient)\n",
    "    parser.add_argument('--no_start_run', type=int, default=no_start_run)\n",
    "    parser.add_argument('--graphlet_size',type=int,default=4, help='Maximal graphlet size.')\n",
    "    parser.add_argument('--label', type=str, default=label)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    return(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fce83eb5",
   "metadata": {
    "id": "5b9ce8ff"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(args)\n",
    "\n",
    "#******* device*******\n",
    "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    print(device)   \n",
    "    print(datetime.now())\n",
    "#*******dataset*******\n",
    "    if args.dataset == 'ogb':\n",
    "        dataset = PygLinkPropPredDataset(name=args.dataset_name)\n",
    "        data = dataset[0]\n",
    "        split_edge = dataset.get_edge_split()\n",
    "        pos_train_edge = split_edge['train']['edge']\n",
    "        split_edge['train']['edge_neg'] = negative_sampling(data.edge_index, num_nodes=data.num_nodes,\n",
    "            num_neg_samples=pos_train_edge.size(0)).T\n",
    "        \n",
    "        edge_weight = torch.ones(data.edge_index.size(1), dtype=int)\n",
    "        A = ssp.csr_matrix((edge_weight, (data.edge_index[0], data.edge_index[1])), \n",
    "                           shape=(data.num_nodes, data.num_nodes))\n",
    "        n_v = data.num_nodes\n",
    "        n_e = data.num_edges\n",
    "        num_subgraph_nodes = round((2*n_e/n_v)*(1+((2*n_e)/((n_v)*(n_v-1)))))#PLACN\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif args.dataset == 'other':\n",
    "        data_train, data_test, A, A_train, A_test, edge_pos_train,\\\n",
    "        edge_neg_train, edge_pos_test, edge_neg_test, num_subgraph_nodes,\\\n",
    "        sub_feature_train_matrix, sub_adj_train_matrix, sub_feature_test_matrix,\\\n",
    "        sub_adj_test_matrix, num_nodes = datasets()\n",
    "\n",
    "        \n",
    "        split_edge = {'train': {'edge':edge_pos_train, \n",
    "                                  'edge_neg':edge_neg_train},\n",
    "                                  'test':{'edge':edge_pos_test, \n",
    "                                  'edge_neg':edge_neg_test} }\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    data_train = data_train.to(device)\n",
    "    data_test = data_test.to(device)\n",
    "\n",
    "#*******create models*******    \n",
    "    \n",
    "    if args.GNN_type == 'gcn':\n",
    "        model = GCN(len(sub_feature_train_matrix[0][0]), args.GNN_hidden_channels, args.GNN_out_channels, \n",
    "                               args.GNN_num_layers, args.dropout).to(device)\n",
    "    if args.GNN_type == 'sage':\n",
    "        model = SAGE(data_train.x.shape[1], args.GNN_hidden_channels, args.GNN_out_channels, \n",
    "                           args.GNN_num_layers, args.dropout).to(device)\n",
    "    if args.GNN_type == 'autoencoder':\n",
    "        model = GAE(GAutoEncoder(data_train.x.shape[1], args.GNN_hidden_channels, args.GNN_out_channels,\n",
    "                                   args.GNN_num_layers, args.dropout).to(device))\n",
    "        \n",
    "   \n",
    "    model_predictor = predictor_model(args.GNN_out_channels, args.linear_hidden_channels, \n",
    "                                          args.linear_num_layers, args.dropout).to(device)\n",
    "    \n",
    "    if args.subgraph_feature_type == 'CNN':\n",
    "        model_subgraph = subg2vec_model(num_subgraph_nodes, args.GNN_out_channels, args.subg2vec_hidden_channels, \n",
    "                                          args.subg2vec_out_channels, args.dropout).to(device)\n",
    "    \n",
    "    coeff = args.coefficient\n",
    "\n",
    "#*******train and test model*******    \n",
    "    if args.dataset == 'ogb':\n",
    "        print('OGB')\n",
    "            \n",
    "#**************            \n",
    "    elif args.dataset == 'other':\n",
    "    \n",
    "        for run in range(args.no_start_run , 1 + args.runs):\n",
    "            \n",
    "            model.reset_parameters()\n",
    "            model_predictor.reset_parameters()\n",
    "            if args.subgraph_feature_type == 'CNN':\n",
    "                model_subgraph.reset_parameters()\n",
    "                \n",
    "            optimizer = torch.optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "            optimizer_predictor = torch.optim.Adam(list(model_predictor.parameters()), lr=args.lr)\n",
    "            if args.subgraph_feature_type == 'CNN':\n",
    "                optimizer_subgraph = torch.optim.Adam(list(model_predictor.parameters()), lr=args.lr)\n",
    "\n",
    "\n",
    "            min_loss = 1000\n",
    "            losses = []\n",
    "            \n",
    "            for epoch in range(1, 1 + args.epochs):\n",
    "                print(f'Epoch: {epoch:02d}, ')\n",
    "#*******train*******\n",
    "                print(datetime.now())\n",
    "\n",
    "                if args.subgraph_feature_type == 'CNN':\n",
    "                    loss = train(model, model_predictor, data_train, A_train, split_edge, sub_adj_train_matrix,\n",
    "                                 sub_feature_train_matrix, optimizer, optimizer_predictor, args.batch_size,\n",
    "                                 num_subgraph_nodes, model_subgraph, optimizer_subgraph)\n",
    "                elif args.subgraph_feature_type == 'NDP':\n",
    "                    loss = train(model, model_predictor, data_train, A_train, split_edge, sub_adj_train_matrix,\n",
    "                                 sub_feature_train_matrix, optimizer, optimizer_predictor, args.batch_size,\n",
    "                                 num_subgraph_nodes)\n",
    "\n",
    "                print(datetime.now())\n",
    "\n",
    "\n",
    "\n",
    "                min_loss = loss\n",
    "                       \n",
    "                if epoch % coeff == 0 :\n",
    "\n",
    "                    torch.save(model.state_dict(), 'models/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                               %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                 args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    torch.save(model_predictor.state_dict(), 'models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                               %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                 args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    if args.subgraph_feature_type == 'CNN':\n",
    "\n",
    "                        torch.save(model_subgraph.state_dict(), 'models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                   %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                     args.subgraph_feature_type, args.dist_type, epoch, run))\n",
    "\n",
    "                    losses.append(min_loss)\n",
    "\n",
    "                if epoch % args.eval_steps == 0:\n",
    "\n",
    "                    results = []\n",
    "                    \n",
    "                    for e in range(1,args.epochs+1):\n",
    "                        if e%coeff == 0:\n",
    "#*******test*******                               \n",
    "                            model.load_state_dict(torch.load('models/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                               %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                 args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                            model_predictor.load_state_dict(torch.load('models/%s_%s_%s_model_predictor_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                               %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                 args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                            if args.subgraph_feature_type == 'CNN':\n",
    "                                model_subgraph.load_state_dict(torch.load('models/%s_%s_%s_model_subgraph_%s_%s_%s_epochs%d_run%d.pth' \n",
    "                                       %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                         args.subgraph_feature_type, args.dist_type, e, run)))\n",
    "\n",
    "                                result = test(model, model_predictor, data_train, A_train, data_test, A_test,\n",
    "                                              split_edge, sub_adj_train_matrix,\n",
    "                                              sub_feature_train_matrix, sub_adj_test_matrix, sub_feature_test_matrix,\n",
    "                                              args.batch_size, num_subgraph_nodes, model_subgraph)\n",
    "                            elif args.subgraph_feature_type == 'NDP':\n",
    "                                result = test(model, model_predictor, data_train, A_train, data_test, A_test,\n",
    "                                              split_edge, sub_adj_train_matrix,\n",
    "                                              sub_feature_train_matrix, sub_adj_test_matrix, sub_feature_test_matrix,\n",
    "                                              args.batch_size, num_subgraph_nodes)\n",
    "\n",
    "\n",
    "\n",
    "                            results.append(result)\n",
    "                            \n",
    "                            pred_train = result['train']['pred']\n",
    "                            pred_test = result['test']['pred']\n",
    "\n",
    "                            y_train = result['train']['y']\n",
    "                            y_test = result['test']['y'] \n",
    "                            \n",
    "                            accu_train = result['train']['accu_train']\n",
    "                            accu_test = result['test']['accu_test']\n",
    "\n",
    "                            recall_train = result['train']['recall_train']\n",
    "                            recall_test = result['test']['recall_test']\n",
    "\n",
    "                            precision_train = result['train']['precision_train']\n",
    "                            precision_test = result['test']['precision_test']\n",
    "\n",
    "                            f1_train = result['train']['f1_train']\n",
    "                            f1_test = result['test']['f1_test']\n",
    "\n",
    "                            avg_precision_train = result['train']['avg_precision_train']\n",
    "                            avg_precision_test = result['test']['avg_precision_test']\n",
    "\n",
    "                            pr_auc_train = result['train']['pr_auc_train']\n",
    "                            pr_auc_test = result['test']['pr_auc_test']\n",
    "                            \n",
    "                            roc_auc_train = result['train']['roc_auc_train']\n",
    "                            roc_auc_test = result['test']['roc_auc_test']\n",
    "\n",
    "\n",
    "                            precision_list_train = result['train']['precision_list_train']\n",
    "                            recall_list_train = result['train']['recall_list_train']\n",
    "\n",
    "                            precision_list_test = result['test']['precision_list_test']\n",
    "                            recall_list_test = result['test']['recall_list_test']\n",
    " \n",
    "    #*******print results*******                                \n",
    "                                \n",
    "                            i = (int(e/coeff))-1\n",
    "                            with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.csv' \n",
    "                               %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                 args.subgraph_feature_type, args.dist_type, e, run), mode='w') as csv_file:\n",
    "                                fieldnames = ['loss', 'accu_train', 'accu_test', 'recall_train',\n",
    "                                              'recall_test', 'precision_train', 'precision_test',\n",
    "                                             'f1_train', 'f1_test', 'avg_precision_train', \n",
    "                                              'avg_precision_test', 'pr_auc_train', 'pr_auc_test', \n",
    "                                             'precision_list_train', 'recall_list_train',\n",
    "                                             'precision_list_test', 'recall_list_test',\n",
    "                                             'roc_auc_train', 'roc_auc_test']\n",
    "                                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                                writer.writeheader()\n",
    "                                writer.writerow({'loss':losses[i], 'accu_train': accu_train,  \n",
    "                                                 'accu_test': accu_test, 'recall_train':recall_train, \n",
    "                                                 'recall_test':recall_test, 'precision_train':precision_train,\n",
    "                                                 'precision_test':precision_test,\n",
    "                                                 'f1_train':f1_train,'f1_test':f1_test,\n",
    "                                                 'avg_precision_train':avg_precision_train,\n",
    "                                                 'avg_precision_test':avg_precision_test,\n",
    "                                                 'pr_auc_test':pr_auc_test, \n",
    "                                                 'precision_list_train':precision_list_train, \n",
    "                                                 'recall_list_train':recall_list_train,\n",
    "                                                 'precision_list_test':precision_list_test, \n",
    "                                                 'recall_list_test':recall_list_test,\n",
    "                                                 'roc_auc_train': roc_auc_train, 'roc_auc_test': roc_auc_test})\n",
    "                                \n",
    "                                \n",
    "                                with open('results/pred_%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.csv' \n",
    "                                   %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                                     args.subgraph_feature_type, args.dist_type, e, run), mode='w') as csv_file:\n",
    "                                    fieldnames = ['pred_train', 'pred_test', 'y_train', 'y_test']\n",
    "                                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                                    writer.writeheader()\n",
    "                                    writer.writerow({ 'pred_train': pred_train.tolist(), 'pred_test':pred_test.tolist(), \n",
    "                                                     'y_train':y_train.tolist(), 'y_test':y_test.tolist() })\n",
    "\n",
    "\n",
    "                            print(f'Best Result in epochs {e:02d} and Run {run:02d} ')\n",
    "                            print('---')\n",
    "\n",
    "\n",
    "                            print(f'Loss_model: {losses[i]:.4f}')\n",
    "\n",
    "                            print(f'Accu_Train: {accu_train:.4f},     '\n",
    "                                  f'Accu_Test: {accu_test:.4f}')\n",
    "\n",
    "                            print(f'Recall_Train: {recall_train:.4f},     '\n",
    "                                  f'Recall_Test: {recall_test:.4f}')\n",
    "\n",
    "                            print(f'Precision_Train: {precision_train:.4f},     '\n",
    "                                  f'Precision_Test: {precision_test:.4f}')\n",
    "\n",
    "                            print(f'F1_Score_Train: {f1_train:.4f},     ' \n",
    "                                  f'F1_Score_Test: {f1_test:.4f}')\n",
    "\n",
    "                            print(f'Avrage_precision_Train: {avg_precision_train:.4f},     ' \n",
    "                                  f'Avrage_precision_Test: {avg_precision_test:.4f}')\n",
    "\n",
    "                            print(f'Precision-Recall_AUC_Train: {pr_auc_train:.4f},     '\n",
    "                                  f'Precision-Recall_AUC_Test: {pr_auc_test:.4f}')\n",
    "                            \n",
    "                            print(f'ROC_AUC_Train: {roc_auc_train:.4f},     '\n",
    "                                  f'ROC_AUC_Test: {roc_auc_test:.4f}')\n",
    "\n",
    "                            print('---')\n",
    "\n",
    "\n",
    "\n",
    "        results=[]\n",
    "\n",
    "        for r in range(1,args.runs+1):\n",
    "            result=[]\n",
    "            for e in range(1,args.epochs+1):\n",
    "\n",
    "                if (e)%args.coefficient==0:\n",
    "                    df = pd.read_csv('results/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d.csv' \n",
    "                           %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                             args.subgraph_feature_type, args.dist_type, e, r))\n",
    "                    result.append([df['loss'].item(), df['accu_train'].item(), df['accu_test'].item(), df['recall_train'].item(),\n",
    "                                      df['recall_test'].item(), df['precision_train'].item(), df['precision_test'].item(),\n",
    "                                     df['f1_train'].item(), df['f1_test'].item(), df['avg_precision_train'].item(),\n",
    "                                      df['avg_precision_test'].item(), df['pr_auc_train'].item(), df['pr_auc_test'].item(),\n",
    "                                      df['roc_auc_train'].item(), df['roc_auc_test'].item()])\n",
    "            results.append(result)\n",
    "        sum_results = torch.zeros([15])\n",
    "\n",
    "        for r in range(args.runs):\n",
    "            sum_results = sum_results+torch.tensor(results[r])\n",
    "\n",
    "        final_result_model = sum_results/args.runs\n",
    "\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Final Results')\n",
    "        print('---')\n",
    "\n",
    "        for e in range(1,args.epochs+1):\n",
    "            if e%coeff == 0:\n",
    "\n",
    "                i = int(e/coeff)-1\n",
    "\n",
    "                print(f'Final Results Epoch: {e:02d}')\n",
    "                print('---')\n",
    "                print(f'Loss_model: {final_result_model[i][0].item():.4f} ')\n",
    "\n",
    "                print(f'Accu_Train: {final_result_model[i][1].item():.4f},     '\n",
    "                      f'Accu_Test: {final_result_model[i][2].item():.4f}')\n",
    "\n",
    "                print(f'Recall_Train: {final_result_model[i][3].item():.4f},     '\n",
    "                      f'Recall_Test: {final_result_model[i][4].item():.4f}')\n",
    "\n",
    "                print(f'Precision_Train: {final_result_model[i][5].item():.4f},     '\n",
    "                      f'Precision_Test: {final_result_model[i][6].item():.4f}')\n",
    "\n",
    "                print(f'F1_Score_Train: {final_result_model[i][7].item():.4f},     ' \n",
    "                      f'F1_Score_Test: {final_result_model[i][8].item():.4f}')\n",
    "\n",
    "                print(f'Avrage_precision_Train: {final_result_model[i][9].item():.4f},     ' \n",
    "                      f'Avrage_precision_Test: {final_result_model[i][10].item():.4f}')\n",
    "\n",
    "                print(f'Precision-Recall_AUC_Train: {final_result_model[i][11].item():.4f},     '\n",
    "                      f'Precision-Recall_AUC_Test: {final_result_model[i][12].item():.4f}')\n",
    "                \n",
    "                print(f'ROC_AUC_Train: {final_result_model[i][13].item():.4f},     '\n",
    "                      f'ROC_AUC_Test: {final_result_model[i][14].item():.4f}')\n",
    "                print('---')\n",
    "\n",
    "\n",
    "                with open('results/%s_%s_%s_model_%s_%s_%s_epochs%d_run%d_final.csv' \n",
    "                            %(args.dataset_name, args.feature_type, args.GNN_type, args.subgraph_type,\n",
    "                            args.subgraph_feature_type, args.dist_type, e, args.runs) , mode='w') as csv_file:\n",
    "                    fieldnames = ['loss', 'accu_train', 'accu_test', 'recall_train',\n",
    "                                  'recall_test', 'precision_train', 'precision_test',\n",
    "                                 'f1_train', 'f1_test', 'avg_precision_train',\n",
    "                                  'avg_precision_test', 'pr_auc_train', 'pr_auc_test',\n",
    "                                  'roc_auc_train', 'roc_auc_test']\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    writer.writeheader()\n",
    "                    writer.writerow({'loss':final_result_model[i][0].item(), \n",
    "                                     'accu_train': final_result_model[i][1].item(), \n",
    "                                     'accu_test': final_result_model[i][2].item(),  \n",
    "                                     'recall_train':final_result_model[i][3].item(), \n",
    "                                     'recall_test':final_result_model[i][4].item(), \n",
    "                                     'precision_train':final_result_model[i][5].item(),\n",
    "                                     'precision_test':final_result_model[i][6].item(),\n",
    "                                     'f1_train':final_result_model[i][7].item(),\n",
    "                                     'f1_test':final_result_model[i][8].item(),\n",
    "                                     'avg_precision_train':final_result_model[i][9].item(),\n",
    "                                     'avg_precision_test':final_result_model[i][10].item(), \n",
    "                                     'pr_auc_train':final_result_model[i][11].item(),\n",
    "                                     'pr_auc_test':final_result_model[i][12].item(),\n",
    "                                     'roc_auc_train':final_result_model[i][13].item(),\n",
    "                                     'roc_auc_test':final_result_model[i][14].item(),\n",
    "                                     \n",
    "                                     })\n",
    "        print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4489a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = parse_arguments(dataset='other', dataset_name='Karate', subgraph_type='DIS', dist_type='cos',\n",
    "                           feature_type='node2vec', subgraph_feature_type='NDP', GNN_type='gcn', epochs=200,\n",
    "                           eval_steps=200, no_start_run=1, runs=1, coefficient=50)\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
